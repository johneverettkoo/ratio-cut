---
title: 'Notes on the SDP relaxation of $k$-means'
output: pdf_document
# output: html_document
# geometry: "left=1cm,right=1cm,top=1cm,bottom=1.5cm"
urlcolor: blue
header-includes:
- \usepackage{float}
- \usepackage{amsmath}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      comment = NA, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.pos = 'H', 
                      fig.align = 'center', 
                      fig.height = 5, 
                      fig.width = 8, 
                      fig.dpi = 300)
options(xtable.comment = FALSE, 
        xtable.table.placement = 'H')
```

\newcommand{\tr}{\text{Tr}}
\newcommand{\diag}{\text{diag}}

```{r}
library(ggplot2) 

source('http://pages.iu.edu/~mtrosset/Courses/675/graph.r')
import::from(magrittr, `%>%`)
source('~/dev/ratio-cut/functions.R')
source('~/dev/ratio-cut/sdp-functions.R')

doMC::registerDoMC(8)
theme_set(theme_bw())
```

# The SDP relaxation of $k$-means

Iguchi et al.[^iguchi] formulated a semidefinite progmming approach to 
$k$-means as follows[^notation]:

[^iguchi]: https://arxiv.org/abs/1505.04778

[^notation]: the notation is slightly different here

$$\begin{split} 
  \arg\max_Z & -\tr(D_2 Z) \\
  \text{s.t. } & \tr(Z) = k \\
  & Ze = e \\
  & Z \geq 0 \text{ element-wise} \\
  & Z \text{ is positive semidefinite}
\end{split}$$

Where 

* $D_2 = [d_{ij}] = [||x_i - x_j||^2]$
* $x_1, ..., x_n \in \mathbb{R}^q$
* The number of clusters, $k$, is known
* $e = \begin{bmatrix} 1 \\ \vdots \\ 1 \end{bmatrix} \in \mathbb{R}^q$

Note that without the SDP relaxation, we have a rigid structure for $Z$ where 
$z_{ij} = \begin{cases}
  n_k^{-1} & x_i, x_j \text{ in same cluster } k \\
  0 & \text{else}
\end{cases}$

# Equating the trace formulation of $k$-means to kernel $k$-means

We can see that the data matrix 
$X = \begin{bmatrix} x_1^\top \\ \vdots \\ x_n^\top \end{bmatrix}$ is not 
explicitly in the objective, although squared Euclidean distances are. We can 
rewrite this as a kernel formulation by noting that $D_2 = \kappa(B)$ where 
$B$ is a kernel matrix[^math]:

[^math]: In the following steps we use the fact that $Z$ is symmetric, 
$Ze = e$, $e^\top Z = e^\top$, and $e^\top e = n$

$$\begin{split}
  -\tr(D_2 Z) & = -\tr(\kappa(B) Z) \\
  & = -\tr((b e^\top - 2 B + e b^\top) Z) \\
  & = 2 \tr(B Z) - \tr(b e^\top Z) - \tr(e b^\top Z) \\
  & = 2 \tr (B Z) - \tr(b e^\top) - \tr(Z e b^\top) \\
  & = 2 \tr (B Z) - \tr(b e^\top) - \tr(e b^\top) \\
  & = 2 \tr (B Z) - 2 \tr(b e^\top) \\
  & = 2 \tr (B Z) - 2 \tr(B)
\end{split}$$

... where $b = \diag(B)$, the vector of diagonal entries of $B$. Note that if 
we think of $B$ as a weight matrix for an undirected graph, $\tr(B) = 0$. 
Similarly, if we impose that the diagonal entries of $B$ are equal to $1$ 
(e.g., $B$ is a correlation matrix), then $\diag(B) = n$. Either way, 
$\tr(B)$ does not depend on $Z$, so we can ignore it in the objective, and we 
can see that $-\arg\max_Z \tr(D_2 Z) = \arg\max_Z \tr(B Z)$, which is just the 
typical kernel formulation of $k$-means: 

$$\begin{split}
  \arg\max_Z & \tr(B Z) \\
  \text{s.t. } & \tr(Z) = k \\
  & z_{ij} = \begin{cases} 
    n_k^{-1} & x_i, x_j \text{ in same cluster } k \\
    0 & \text{else} 
  \end{cases}
\end{split}$$

Similarly, we can go from a kernel formulation of $k$-means to one based on 
squared Euclidean distances by noting that $D_2 = \tau(B)$. For simplicity of 
notation, we will rewrite $\arg\max_x f(x) = \arg\max_x 2 f(x)$.

$$\begin{split}
  2\tr(B Z) & = 2\tr(\tau(D_2) Z) \\
  & = \tr(-P D_2 P Z) \\
  & = -\tr((I - n^{-1} e e^\top) D_2 (I - n^{-1} e e^\top) Z) \\
  & = -\tr((D_2 - n^{-1} D_2 e e^\top - 
    n^{-1} e e^\top D_2 + n^{-2} e e^\top e e^\top D_2) Z) \\
  & = -\tr(D_2 Z) + n^{-1} \tr(D_2 e e^\top Z) + 
    n^{-1} \tr(e e^\top D_2 Z) - n^{-1} \tr (e e^\top D_2 Z) \\
  & = -\tr(D_2 Z) + 2 n^{-1} \tr(D_2 e e^\top) - n^{-1} \tr(D_2 e e^\top) \\
  & = -\tr(D_2 Z) + n^{-1} \tr(D_2 e e^\top)
\end{split}$$

Since the second and third terms do not depend on $Z$, we can discard them, and 
we get $\arg\max_Z \tr(B Z) = \arg\max_Z -\tr(D_2 Z)$.[^notes]

[^notes]: We can rewrite 
$\tr(D_2 e e^\top) = \sum_{i,j} d^2_{ij} = 2\sum_{i < j} d^2_{ij}$

# Equating the SDP relaxation of $k$-means to the SDP relaxation of ratio cut

The ratio cut objective is:

$$\arg\min_Z \tr(L Z)$$

where $L$ is the combinatorial graph Laplacian and $Z$ has the same structure 
as before. If we relax the optimization problem by not enforcing $Z$ to have 
this structure, we can see that:

$$\arg\min_Z \tr(L Z) = \arg\max_Z \tr(L^\dagger Z)$$

where $L^\dagger$ is the generalized inverse of $L$. Since $L^\dagger$ is 
positive semidefinite, it can be thought of as a kernel matrix, and we can 
apply the $\tau(\cdot)$ transformation to it to obtain $D_2$. In this case, 
$D_2$ is the expected commute time of the graph that generated $L$.

The argmin and argmax equivalence is not true in general if we force $Z$ to 
have the structure that we want. It also is not true if we apply the 
SDP constraints (namely $Z \geq 0$ element-wise). One question of interest 
is under what conditions can we equate the two objectives under the SDP 
constraints.

# Examples

### Example 1

Here we look at a case where 
$\arg\min_Z \tr(L Z) = \arg\max_Z \tr(L^\dagger Z)$ under the SDP restrictions 
($Ze = e$, $\tr(Z) = k$, $Z$ is positive semidefinite, $Z \geq 0$ 
element-wise). In fact, in this example, not only do the two problems have 
the same solution, the solution coincides with the solution to the unrelaxed 
ratio cut problem. 

Here we have a very simple graph with just six vertices. The "intuitive cut" 
for $k = 2$ is the $3-4$ cut, which happens to also be the solution to the 
(fully constrained) ratio cut problem.

```{r}
k <- 2
```

```{r, fig.height = 1, fig.width = 1}
W <- rbind(c(1, 1, 1, 0, 0, 0), 
           c(1, 1, 1, 0, 0, 0), 
           c(1, 1, 1, 0, 0, 0), 
           c(0, 0, 0, 1, 1, 1), 
           c(0, 0, 0, 1, 1, 1), 
           c(0, 0, 0, 1, 1, 1))
W[3, 4] <- W[4, 3] <- 1
qgraph::qgraph(W, layout = 'circular')
L <- graph.laplacian(W)
L.dagger <- MASS::ginv(L)
```

The RatioCut-SDP solution (Ling and Strohmer)[^rcsdp] yields:

```{r echo = TRUE}
rc.sdp(L, k = 2)$Z %>% 
  MASS::fractions()
```

[^rcsdp]: https://arxiv.org/abs/1806.11429

The $k$-means SDP solution using $L^\dagger$ as a kernel matrix (adapted from 
Peng and Wei)[^kmeanssdp] yields:

```{r echo = TRUE}
kmeans.sdp(L.dagger, k = 2)$Z %>% 
  MASS::fractions()
```

[^kmeanssdp]: http://www.optimization-online.org/DB_FILE/2005/04/1114.pdf

### Example 2

Let's try the epsilon graph example from the text. This graph has four vertices 
connected in series by three edges. The $2-3$ and $3-4$ edges have weight 1 but 
the $1-2$ edge has weight $\epsilon \in (0, 1)$.

```{r fig.height = 1, fig.width = 1}
eps <- .5
W <- rbind(c(0, eps, 0, 0), 
           c(eps, 0, 1, 0), 
           c(0, 1, 0, 1), 
           c(0, 0, 1, 0))
qgraph::qgraph(W)
```

Recall that the graph Laplacian is of the form  
$\begin{bmatrix} 
  \epsilon & -\epsilon & 0 & 0 \\
  -\epsilon & 1 + \epsilon & -1 & 0 \\
  0 & -1 & 2 & -1 \\
  0 & 0 & -1 & 1
\end{bmatrix}$. 

We previously showed that when $\epsilon \in (0, 0.75)$, the optimal ratio 
cut is the $1-2$ cut, and when $\epsilon \in (0.75, 1)$, the optimal ratio cut 
is the $2-3$ cut. However, for kernel $k$-means, the optimal clustering is 
$\{\{1\}, \{2, 3, 4\}\}$ when $\epsilon \in (0, 0.6)$ and 
$\{\{1, 2\}, \{3, 4\}\}$ when $\epsilon \in (0.6, 1)$. One thing that might be 
of interest is whether RatioCut-SDP and $k$-means-SDP yields the same results. 

```{r, cache = TRUE, fig.height = 2, fig.width = 8}
eps.vec <- seq(.5, .8, .01)
n <- 4

out.df <- plyr::ldply(eps.vec, function(e) {
  L <- degenerate.graph.laplacian(e)
  L.dagger <- MASS::ginv(L)
  rc.clust <- rc.sdp(L, k)$cluster - 1
  kmeans.clust <- kmeans.sdp(L.dagger, k)$cluster - 1
  rc.clust <- ifelse(sum(rc.clust) == n / 2, 
                     '((1, 2), (3, 4))', 
                     '((1), (2, 3, 4))')
  kmeans.clust <- ifelse(sum(kmeans.clust) == n / 2,
                         '((1, 2), (3, 4))', 
                         '((1), (2, 3, 4))')
  dplyr::tibble(epsilon = e, 
                rc.clust = rc.clust,
                kmeans.clust = kmeans.clust)
}, .parallel = TRUE)

out.df %>% 
  dplyr::mutate(rc.clust = factor(rc.clust),
                kmeans.clust = factor(kmeans.clust)) %>% 
  ggplot() + 
  geom_line(aes(x = epsilon, y = rc.clust, colour = 'RC-SDP'),
            size = 2, alpha = .5) + 
  geom_line(aes(x = epsilon, y = kmeans.clust, colour = 'k-means-SDP'),
            size = 2, alpha = .5) + 
  labs(y = 'clustering', x = expression(epsilon),
       colour = NULL) + 
  geom_vline(xintercept = .6) + 
  geom_vline(xintercept = .75) + 
  scale_colour_brewer(palette = 'Set1')
```

Interestingly, both RatioCut-SDP and $k$-means-SDP fail to find the correct 
$\epsilon$ where the optimal cut switches from $1-2$ to $2-3$. 

### Example 3

Here we look at the "spiral graph":

```{r, fig.width = 3, fig.height = 3}
W <- load.double.spiral()
n <- nrow(W)
L <- graph.laplacian(W)
L.dagger <- MASS::ginv(L)

qgraph::qgraph(W)
```

We know that the optimal ratio cut clustering is 
$\{\{1, ..., 100\}, \{101, ..., 200\}\}$. This also happens to coincide with 
the optimal $k$-means clustering. We also know that the spectral clustering 
relaxation of ratio cut fails to provide the optimal ratio cut (even when 
the $k$-means rounding step is initialized with the correct clustering). 

Applying RatioCut-SDP to this graph:

```{r, fig.width = 4, fig.height = 3, cache = TRUE}
init.clust <- c(rep(1, 100), rep(2, 100))
rc.clust <- rc.sdp(L, k, init.clust = init.clust)

qgraph::qgraph(W, groups = factor(rc.clust$cluster))
```

Here we see that RatioCut-SDP makes the same mistake the spectral clustering 
relaxation of ratio cut makes. We can also try $k$-means SDP:

```{r, fig.width = 4, fig.height = 3, cache = TRUE}
kmeans.clust <- kmeans.sdp(L.dagger, k, init.clust = init.clust)

qgraph::qgraph(W, groups = factor(kmeans.clust$cluster))
```

Strangely, $k$-means-SDP makes a similar mistake RatioCut-SDP makes, while 
performing $k$-means on the embedding of $L^\dagger$ is able to find the 
optimal ratio cut (although Lloyd's algorithm runs into local minima issues as 
we add more embedding dimensions, so it's not always practically feasible). 

It's also worth noting that these optimization problems takes a while to solve.

# RatioCut-SDP optimality criterion

Ling and Strohmer outlines the following as being required for RatioCut-SDP to 
find the optimal ratio cut:

$$||D_\delta||_{op} < \frac{\lambda_{k+1}(L_{iso})}{4}$$

where 

* $W_{iso}$ is the weight matrix given that the optimal ratio cut is known 
and the edges of $W$ are cut accordingly
* $W_\delta = W - W_{iso}$, the edges that are to be cut for the optimal ratio 
cut
* $D_{iso}$, $D_\delta$, $L_{iso}$, and $L_\delta$ are the degree and 
combinatorial graph Laplacian matrices constructed from $W_{iso}$ and $W_\delta$
* $||\cdot||_{op}$ is the operator norm
* $\lambda_i(A)$ is the $i^{th}$ eigenvalue of matrix $A$

Examples 1-3 all violate this criterion. 

Intuitively, the criterion says that the within-cluster connectivity must be 
large compared to the magnitude of the betweeen-cluster edges. Note that 
$L_{iso}$ has $k$ zero eigenvalues, one for each subgraph, and 
$\lambda_{k+1}(L_{iso})$ is the smallest "fiedler value" among the subgraphs. 
The fiedler value characterizes how tightly connected a graph (or in this case, 
subgraph) is. The first $k$ eigenvectors of $L_{iso}$ produce exactly the 
solution to the (fully constrained) ratio cut. 

# Examples from the Paper

### Concentric Circles

Here, we will set $r_2 = \frac{3}{2}$, $r_1 = 1$, $n = 10$, $m = 15$. Theorem 
4.1 says if 
$\gamma \leq \bigg(2 + \frac{\log 4m}{\log \frac{m}{2 \pi}}\bigg)^{-1} 
\bigg( \frac{\frac{n^2 \Delta^2}{16} - 1}{2} \bigg)$, 
where $\Delta = \frac{r_2 - r_1}{r_1}$, then a graph constructed using the 
heat kernel with $\sigma^2 = \frac{16 r_1^2 \gamma}{n^2 \log \frac{m}{2 \pi}}$ 
will always be solved by RatioCut-SDP. Here, we will set $\gamma$ directly at 
the boundary of the condition. 

```{r}
r1 <- 1
r2 <- 1.5
n <- 10
m <- floor(n * r2 / r1)
delta <- (r2 - r1) / r1
gam <- (2 + log(4 * m) / log(m / 2 / pi)) ** -1 * 
  ((n ** 2 + delta ** 2) / 16 - 1) / 2
sigma2 <- 16 * r1 ** 2 * gam / n ** 2 / log(m / 2 / pi)
k <- 2

circles.df <- dplyr::bind_rows(
  dplyr::tibble(x = r1 * cos(2 * pi * seq(n) / n),
                y = r1 * sin(2 * pi * seq(n) / n),
                z = 1),
  dplyr::tibble(x = r2 * cos(2 * pi * seq(m) / m),
                y = r2 * sin(2 * pi * seq(m) / m),
                z = 2)
)

ggplot(circles.df) + 
  coord_fixed() + 
  geom_point(aes(x = x, y = y, colour = factor(z))) + 
  theme(legend.position = 'none')

W <- exp(-as.matrix(dist(circles.df)) ** 2 / 2 / sigma2)
qgraph::qgraph(W, 
               layout = 'spring',
               groups = factor(circles.df$z))
```

Looking at the original data and the heat kernel graph, it seems like this 
should be a very "easy" problem to solve. Sure enough, spectral clustering is 
able to solve this very easily. In fact, the 1-dimensional Laplacian eigenmap 
has the points in each cluster almost lying on top of one another:

```{r}
L <- graph.laplacian(W)
L.dagger <- MASS::ginv(L)
L.eigen <- eigen(L)
H.hat <- L.eigen$vectors[, seq(n + m, n + m - k + 1)]
clustering <- kmeans(H.hat, 2)$cluster

ggplot() + 
  coord_fixed() + 
  xlim(1 / sqrt(n + m) -.01, 1 / sqrt(n + m) + .01) + 
  labs(x = expression(u[0]),
       y = expression(u[1])) + 
  geom_point(aes(x = H.hat[, 1], 
                 y = H.hat[, 2], 
                 colour = factor(clustering))) + 
  theme(legend.position = 'none')

table(clustering, circles.df$z)
```

We can also see that $\sigma^2$ is a decreasing function of the sample sizes, 
so we actually get better separation as we increase $n$ (and therefore $m$), 
and spectral clustering is even more able to separate the two clusters. 

```{r}
n <- seq(10, 100)
m <- floor(n * r2 / r1)
delta <- (r2 - r1) / r1
gam <- (2 + log(4 * m) / log(m / 2 / pi)) ** -1 * 
  ((n ** 2 + delta ** 2) / 16 - 1) / 2
sigma2 <- 16 * r1 ** 2 * gam / n ** 2 / log(m / 2 / pi)

ggplot() + 
  geom_point(aes(x = n + m, y = sigma2)) + 
  labs(x = 'number of points',
       y = expression(sigma^2))
```

This illustrates the main criticism of the Ling and Strohmer result: 
the condition under which RatioCut-SDP is guaranteed to work is so stringent 
that it only captures the "easy" problems. Perhaps this points to other methods 
being guaranteed under this condition. 

# Next steps

1. RatioCut-SDP provides $Z \in \mathbb{R}^{n \times n}$, which, under the 
full ratio cut constraints, should be $Z = H H^\top$. We saw that in example 1, 
both RatioCut-SDP and $k$-means-SDP were able to find this exact $Z$, but 
in examples 2 and 3, we needed a "rounding" step where we embedded $Z$ and 
used Lloyd's algorithm to find a clustering. Ling and Strohmer are not 
clear on how this should be handled, and neither are Peng and Wei. 
2. Ling and Strohmer provide some criteria under which RatioCut-SDP is 
guaranteed to find the optimal ratio cut clustering. So far, we haven't checked 
if these criteria hold before running these experiments. 
3. More examples ...
4. Figure out when RatioCut-SDP and $k$-means SDP (using $L^\dagger$ as a 
kernel matrix) coincide.